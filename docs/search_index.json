[["index.html", "Covariate Adjustment in Randomized Trials Chapter 1 The Book 1.1 Using this Book 1.2 Case Studies", " Covariate Adjustment in Randomized Trials Josh Betz, Kelly van Lancker, and Michael Rosenblum Chapter 1 The Book This book is about covariate adjustment for practitioners planning and analyzing randomized trials Marginal Effects: what they are, why we are focusing on them. References on where to find more on conditional effects Goals for covariate adjusted approach We do not want to change the target of inference We want to have the same or better precision (asymptotically) as the unadjusted estimator We want assumptions to be as strict as the unadjusted estimator, if not less strict. We want to provide methods and examples that are broadly applicable: Continuous, binary, ordinal, time-to-event outcomes Applicable to various areas Avoid methods with pathological behavior, such as lack of convergence Potential Benefits: Increased power for same sample size or same power at lower sample size Particularly beneficial in larger trials More ethical, efficient trials Generally supported by regulatory agencies Challenges to Address: Precision gain not known a priori Need a theoretical framework which includes a broad class of estimators Missing data: dropout, not yet collected No ‘off the shelf’ software solutions Reviewer skepticism General guidance and rules for application 1.1 Using this Book This book is meant to provide worked examples to help practitioners apply covariate adjustment in practice. The material will be most accessible to a reader who has a good understanding of probability and statistics (confidence intervals, hypothesis testing), generalized linear models, and survival analysis. Some familiarity with concepts in randomized trials and causal inference is helpful, but not required. In order to make these methods broadly available, we provide worked examples using the R environment for statistical computing, which is free and open source software. While not all users may be familiar with R, we provide example code and links to additional resources to help users understand how to apply it in practice. The data in our examples are simulated from actual randomized trial data, meant to mimic key features such as missingness patterns and the distribution of outcomes and covariates. We will start with an overview of key ideas and findings from research in randomized trial methodology. From there, we will discuss different targets of inference, or estimands, that may be of interest to investigators. Afterwards, we provide a brief overview of using R, and how to install the necessary software to perform covariate adjustment. With the necessary background in place, we will begin with the most simple and commonly used approach for covariate adjustment, the analysis of covariance (ANCOVA), show how this approach can be generalized to binary and other types of outcomes. From there, we will discuss how to address the issues of missing data in baseline covariates and outcomes using doubly robust methods. Since randomized trials are often designed with incomplete and imprecise information, the study design should incorporate pre-planned analyses to determine if the study should continue or be stopped for either success or futility. We discuss how covariate adjustment can be integrated into such analysis plans. Finally, we discuss some recommendations for applying these methods in practice. 1.2 Case Studies The datasets used in our examples are simulated data based on actual randomized trials, with considerable effort spent making the data as realistic as possible. The original study data was used to create regression models for the outcomes of interest and missingness patterns. Next, simulated covariate data were created by resampling the original covariate data, and perturbing the resampled data. Simulated outcome data and missingness patterns were generated using predictions from the outcome regression models, using the simulated covariates as input. 1.2.1 Buprenorphine tapering schedule and illicit opioid use: CTN-03 CTN-03 (NCT00078117) was two-arm a phase III trial to compare two potential tapering schedules of the drug buprenorphine, a pharmacotherapy for opioid dependence. At the time of the study design, there was considerable variation in tapering schedules in practice, and a knowledge gap in terms of the best way to administer buprenorphine to control withdrawal symptoms and give the greatest chance of abstinence at the end of treatment. It was hypothesized that a longer taper schedule would result in greater likelihood of a participant being retained on study and providing opioid-free urine samples at the end of the drug taper schedule. Participants were randomized 1:1 to a 7-day or 28-day taper using stratified block randomization across 11 sites in 10 US cities. Randomization was stratified by the maintenance dose of buprenorphine at stabilization: 8, 16, or 24 mg. The structure of the CTN-03 simulated data is as follows: Baseline Covariates age: Participant age at baseline sex: Participant sex race: Participant race ethnic: Participant ethnicity marital: Participant marital status Randomization Information arm: Treatment Arm stability_dose: Stratification Factor Baseline (_bl) &amp; End-Of-Taper (_eot) Outcomes: arsw_score: Adjective Rating Scale for Withdrawal (ARSW) Score at baseline cows_score: Clinical Opiate Withdrawal Scale (COWS) Score at baseline cows_category: COWS Severity Category - Ordinal vas_crave_opiates: Visual Analog Scale (VAS) - Self report of opiate cravings vas_current_withdrawal: Visual Analog Scale (VAS) - Current withdrawal symptoms vas_study_tx_help: Visual Analog Scale (VAS) - Study treatment helping symptoms uds_opioids: Urine Drug Screen Result - Opioids uds_oxycodone: Urine Drug Screen Result - Oxycodone uds_any_positive: Urine Drug Screen - Any positive result 1.2.2 Functional Outcome in Hemorrhagic Stroke: MISTIE III Hemorrhagic stroke occurs when a blood vessel in the brain ruptures, causing a bleed inside the skull. The bleeding from an ICH can occur in the brain tissue itself (an intracerebral hemorrhage, or ICH), or in the fluid-filled channels in the brain (an intraventricular hemorrhage, or IVH). The MISTIE III trial (NCT01827046) was a phase III study comparing a minimally invasive surgical intervention to conventional medical management for the treatment of spontaneous, non-traumatic ICH. In this study, participants were randomized 1:1 to receive either standard-of-care medical management or a minimal invasive surgery with Alteplase for ICH removal. Outcomes were measured at 30, 180, and 365-days post-randomization using the Modified Rankin Scale (MRS), which measures functional outcome on a scale ranging from 0 (no residual symptoms) to 6 (death). The MRS was collapsed into a binary variable, representing a score of 0-3 (no symptoms to moderate disability but able to walk without assistance) or 4-6 (unable to walk or attend to daily activities without assistance to death). Survival was also assessed, with patients administratively censored on the date of their final MRS assessment. The data from MISTIE III was used to create a synthetic dataset for educational purposes. Baseline covariates include demographics, medications and comorbidities, characteristics of the stroke (location of the ICH lesion, the size of the ICH and IVH lesions on CT scans), and neurological status on presentation to the hospital (the Glasgow Coma Scale, or GCS). In addition to the longitudinal measures of the MRS at 30-, 180-, and 365-days post randomization, mortality data are included. The structure of the data is as follows: sim_participant_id: Patient id Baseline Covariates age: Age in years male: male sex hx_cvd: cardiovascular disease history hx_hyperlipidemia: hyperlipidemia history on_anticoagulants: on anticoagulant medication on_antiplatelets: on antiplatelet medication ich_location: intracerebral hemorrhage location: (Lobar, Deep) ich_s_volume: intracerebral hemorrhage volume on stability scan ivh_s_volume: intraventricular hemorrhage volume on stability scan gcs_category: presenting Glasgow Coma Score (GCS) Treatment: arm: treatment arm ich_eot_volume: intracerebral hemorrhage volume on end-of-treatment scan Outcome: mrs_30d: MRS at 30 days (0-3, 4, 5, 6) mrs_30d_complete: MRS at 30 days if no data were missing mrs_180d: MRS at 180 days (0-2, 3, 4, 5, 6) mrs_180d_complete: MRS at 180 days if no data were missing mrs_365d: MRS at 365 days (0-1, 2, 3, 4, 5, 6) mrs_365d_complete: MRS at 365 days if no data were missing days_on_study: days until death or administrative censoring died_on_study: participant died (1) or is censored (0) "],["introduction.html", "Chapter 2 Introduction 2.1 Notation and Assumptions 2.2 The Analysis of Covariance (ANCOVA) 2.3 Generalizing the ANCOVA: G-computation", " Chapter 2 Introduction In this section, we will lay out the notation for the following chapters, discuss what assumptions are required for these methods to be applied, and survey the literature on covariate adjustment. 2.1 Notation and Assumptions Let \\(A\\) denote a binary treatment assignment: \\(A = 1\\) indicates assignment to receive the treatment of interest, and \\(A = 0\\) indicates assignment to the control or comparator group. Let \\(Y\\) denote the outcome of interest, and \\(X\\) denote a vector of baseline covariates. If stratified randomization is used, let \\(X_{S}\\) denote the stratification variables, and \\(X_{\\bar{S}}\\) denote the other baseline covariates, and \\(X = (X_{S}, X_{\\bar{S}})\\). We assume that treatment assignment is independent of the baseline covariates, i.e. \\(A \\perp X\\), or if stratified randomization is used, that the treatment assignment is conditionally independent of the other covariates given the stratification variables, i.e. \\(A \\perp\\!\\!\\!\\perp X_{\\bar{S}} \\vert X_{S}\\). Each participant’s data is assumed to be independent, identically distributed (IID) draws from an unknown distribution \\(P(X, A, Y)\\). In order to include covariate information in an analysis, we have to specify how these covariates relate to the outcome in a regression model, which models a conditional distribution of the outcome as as a function of the covariates. In most circumstances, the validity of our inference from regression models depends on how close the regression model’s specification reflects the true data generation mechanism, which is almost always unknown. This may make investigators understandably reluctant to use covariate adjusted methods, as they do not want the validity of the analysis to depend on assumptions of a correctly specified model. What may be surprising is that there are covariate adjustment methods that provide valid estimates of the treatment effect, even if the models used to obtain these estimates are arbitrarily misspecified. These estimators also can have precision that is equal or better than the unadjusted estimators. 2.2 The Analysis of Covariance (ANCOVA) The ANCOVA is perhaps the best known method for covariate adjustment with a continuous outcome. The simplest version of an ANCOVA is a linear regression of the final outcome \\(Y\\) on the baseline covariate \\(X\\), usually the outcome measured at baseline, and treatment assignment \\(A\\): \\[Y_{i} = \\beta_{0} + \\beta_{X}X_{i} + \\beta_{A}A_{i} + \\epsilon_{i}\\] This regression model assumes that the final outcome is linearly related to the outcome at baseline, with an additive effect of treatment. Even if the true relationship between the final outcome and the baseline covariate is nonlinear, includes interactions, and includes other variables that are omitted from the model, the ANCOVA estimate will provide a consistent estimate of the treatment effect that is as precise or even more precise than an unadjusted estimate. An outcome model that more accurately reflects the data generating mechanism will improve precision, but is not required for a consistent estimate of the outcome. Linear regression involves the conditional mean, but we are interested in a marginal treatment effect. In order to obtain a marginal treatment effect, we need to marginalize (i.e. average over) the variation in the covariates: \\[\\hat{\\mu}_{1} = \\hat{E}[Y \\vert A = 1] = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{E}[Y \\vert A = 1, X]\\] In the case of the simple ANCOVA model with only one covariate, the marginal mean under treatment would be: \\[\\hat{\\mu}_{1} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{E}[Y \\vert A = 1, X] = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{\\beta}_{0} + \\hat{\\beta}_{X}X_{i} + \\hat{\\beta}_{A}) = \\hat{\\beta}_{0} + \\hat{\\beta}_{X}\\bar{X}_{n} + \\hat{\\beta}_{A}\\] Note that this utilizes the covariate data from the entire sample, not just those who were assigned to receive the active treatment. The marginal mean under control would be: \\[\\hat{\\mu}_{0} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{E}[Y \\vert A = 1, X] = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{\\beta}_{0} + \\hat{\\beta}_{X}X_{i}) = \\hat{\\beta}_{0} + \\hat{\\beta}_{X}\\bar{X}_{n}\\] Likewise, our marginal estimate of the mean under control utilizes the covariate data from the entire sample, not just those who were assigned to receive the control treatment. Our estimate of the average treatment effect would be the contrast between these quantities: \\[\\hat{\\theta}_{ATE} = \\hat{\\mu}_{1} - \\hat{\\mu}_{0} = (\\hat{\\beta}_{0} + \\hat{\\beta}_{X}\\bar{X}_{n} + \\hat{\\beta}_{A}) - (\\hat{\\beta}_{0} + \\hat{\\beta}_{X}\\bar{X}_{n}) = \\hat{\\beta}_{A}\\] The procedure for gives us the same result as we would have otherwise used to estimate the treatment effect: the regression coefficient associated with treatment. When there are no treatment-by-covariate interactions and the model does not use a nonlinear link function, the conditional effect and the marginal effect coincide. However, the approach we outlined here is applicable to a wide range of outcome models, including those using link functions, like logistic regression, or models that include treatment-by-covariate interactions. 2.3 Generalizing the ANCOVA: G-computation In general, if we have a regression model \\(\\hat{f}(X, A) = \\hat{E}[Y \\vert A, X]\\), we can obtain the average treatment effect by respectively computing the marginal mean under treatment and control, and taking a contrast between these quantities: \\[\\hat{\\theta}_{ATE} = \\hat{\\mu_{1}} - \\hat{\\mu_{0}} = \\left(\\frac{1}{n} \\sum_{i=1}^{n} \\hat{f}(X, A = 1) \\right) - \\left(\\frac{1}{n} \\sum_{i=1}^{n} \\hat{f}(X, A = 0) \\right)\\] This approach is known as G-computation or the standardization estimator. This allows us to obtain the average treatment effect for regression models which may include interactions or use a nonlinear link function, such as logistic regression. "],["estimands.html", "Chapter 3 Targets of Inference: Estimands 3.1 Estimands for Continuous and Binary Outcomes 3.2 Estimands for Ordinal Outcomes 3.3 Survival/Time-To-Event Outcomes", " Chapter 3 Targets of Inference: Estimands 3.1 Estimands for Continuous and Binary Outcomes 3.1.1 Difference in Means When an outcome is continuous or binary, one meaningful summary may be the mean of the outcome, and a meaningful comparison may be the difference in means estimand: \\[\\theta_{DIM} = E[Y \\vert A = 1] - E[Y \\vert A = 0]\\] This estimand compares the mean outcome in the population of interest if all individuals received the treatment of interest to the the mean outcome in the population of interest if all individuals received the control or comparator intervention. Note that in binary outcomes, this is a difference in proportions (or risk difference) between the population where all individuals received the treatment of interest compared to receiving the control or comparator intervention. 3.1.2 Ratio of Means The Difference in Means gives an absolute measure of an effect. For a relative measure of an effect, such as the relative risk, we can compare the ratio of these means: \\[\\theta_{ROM} = E[Y \\vert A = 1]/E[Y \\vert A = 0]\\] 3.2 Estimands for Ordinal Outcomes Let \\(Y\\) be an ordinal outcome with \\(k\\) ordered categories. For each outcome category \\(j \\in \\{1, \\ldots, K\\}\\), the cumulative distribution function of \\(Y\\) given treatment \\(A\\) is denoted as: \\[Pr \\left\\{Y \\le j\\right\\} = F(j \\vert a)\\] The probability mass function of \\(Y\\) given treatment \\(A\\) is denoted as: \\[Pr \\left\\{Y = j\\right\\} = f(j \\vert a) = F(j \\vert a) - F(j-1 \\vert a)\\] Although this notation involves numeric labels for levels, this is merely to simplify notation. Clarifications will be made as needed when distinguishing between outcomes with and without a numeric levels. 3.2.1 Difference in Mean Utility If the levels of \\(Y\\) have numeric labels, and the mean value of this ordinal variable is meaningful, the difference in means estimand may still be meaningful and useful. Alternatively, if either the labels do not have a numeric interpretation, or the mean of these values is not particularly meaningful, it may be possible to create a meaningful numeric value by assigning ‘utilities’ or ‘weights’ to each level of the outcome. The quantitative and clinical meanings of the difference in means estimator will depend on the utilities assigned to the outcome scale. This allows the difference in means to be used, even if the levels of the outcome are not numeric (e.g. the Glasgow Outcome Scale, ranging from ‘Dead’, ‘Vegetative state’, ‘Severely disabled’, ‘Moderately disabled’, and ‘Good recovery’). Let \\(u(\\cdot)\\) denote a pre-specified mapping from the outcome labels to utility values: \\[ u(Y)= \\begin{cases} u_{1} := \\text{utility of } Y = 1\\\\ u_{2} := \\text{utility of } Y = 2\\\\ \\vdots \\\\ u_{k} := \\text{utility of } Y = k \\end{cases} \\] The utilities will usually be monotone increasing, such that each succesive level of the outcome is associated with equal or better utility. Alternatively, if lower values of the outcome are preferable, utilities will usually be monotone decreasing. Once the utilities have been defined, the estimand is defined as: \\[\\theta_{DIM} = E[u(Y) \\vert A=1] - E[u(Y) \\vert A=0] = \\sum_{i=1}^{k}u(j)\\left(f( j \\vert 1) - f(j \\vert 0)\\right)\\] When all outcomes at or above a threshold \\(t \\in \\{2, \\ldots, k\\}\\) are given a utility of 1, and all others are given a utility of 0, this collapses the ordinal outcome into a binary one. The resulting estimand is the risk difference estimator of the outcome being at or above \\(t\\): \\[ u(Y)= \\begin{cases} 1: &amp; Y \\geq t \\\\ 0: &amp; Y &lt; t \\end{cases} \\] While a risk difference may be more familiar to implement and conceptually easier to interpret, it treats all outcome states either below or above the threshold identically, ignoring potential information in such outcome states. 3.2.2 Mann-Whitney (M-W) Estimand The Mann-Whitney estimand gives the probability that a randomly-selected person assigned to treatment of interest will have an outcome on the same level or a higher level than a randomly-selected person assigned to the comparator group, with ties broken at random: \\[\\theta_{MW} = P(\\tilde{Y} &gt; Y \\vert \\tilde{A} = 1, A = 0) + \\frac{1}{2}P(\\tilde{Y} = Y \\vert \\tilde{A} = 1, A = 0) = \\\\ \\sum_{j=1}^{K} \\left\\{ F(j-1 \\vert 0) + \\frac{1}{2} f(j \\vert 0) \\right\\} f(j \\vert 1)\\] If there is no difference in treatments, we would expect a randomly selected individual from one group to have a higher outcome than a randomly selected individual from the other group about half the time: the null value for this estimand is \\(1/2\\). Note that if higher numerical values indicate worse outcomes, the outcome scale can be reversed prior to analysis, so that the estimand can be interpreted as the probability that a randomly-selected person assigned to treatment of interest will have an outcome as good or better than a randomly-selected person assigned to the comparator group. This estimand addresses a common concern of those choosing between treatment options, and may be easier to communicate to a lay audience. 3.2.3 Log Odds Ratio (LOR) In the case of a binary outcome, the odds ratio of a “good” outcome (\\(Y=1\\)) is \\(OR = odds(Y = 1 \\vert A = 1)/odds(Y = 1 \\vert A = 0)\\): a value greater than 1 indicates a greater likelihood of a “good” outcome in the treatment of interest relative to the comparator group, and the log of the odds ratio will be positive. In the case of an ordinal outcome with categories \\(1, \\ldots, K\\), these categories can be collapsed into \\((K-1)\\) binary outcomes: \\(Y \\le j\\) for \\(j \\in \\{1, \\ldots, (K-1) \\}\\). The odds ratio at threshold \\(j\\) compares the odds of falling at or below level \\(j\\) between the treatment of interest and the comparator group: \\[OR_{j} = \\frac{odds(Y \\le j \\vert A = 1)}{odds(Y \\le j \\vert A = 0)}\\] When this odds ratio is greater than 1, individuals assigned to the treatment of interest are more likely to have outcomes at or below level \\(j\\) than those in the comparator group: the log of this odds ratio will be positive. The log odds ratio estimand combines information across the levels of an ordinal outcome by averaging the log odds of an outcome at or below each threshold across all thresholds of the outcome: \\[\\theta_{LOR} = \\frac{1}{K-1} \\sum_{j=1}^{K-1} log \\left( \\frac{odds(Y \\le j \\vert A = 1)}{odds(Y \\le j \\vert A = 0)} \\right) = \\frac{1}{K-1} \\sum_{j=1}^{K-1} log \\left( \\frac{F(j \\vert 1)/ \\left( 1 - F(j \\vert 1) \\right) } {F(j \\vert 0)/ \\left( 1 - F(j \\vert 0) \\right) } \\right)\\] This estimand is related to the proportional odds logistic regression model, a common parametric model for analyzing ordinal outcomes. In the proportional odds model, a regression coefficient for treatment group gives the increase in the odds of being at or below a given level of the outcome associated with a unit increase in that variable holding all else constant: \\[log(odds(Y \\le j \\vert A)) = logit \\left(P(Y \\le j \\vert A) \\right) = \\alpha_{j} + \\beta A: \\quad j \\in \\{1, \\ldots, (K-1)\\}\\] A positive slope indicates greater likelihood of lower scores in those assigned to receive the treatment of interest relative to the comparator group. The proportional odds assumption involves assuming that the treatment has the same effect across each binary threshold (i.e. that \\(\\beta\\) does not vary across the \\(K-1\\) thresholds). When this assumption holds, the log odds ratio estimand is the same as the coefficient in the proportional odds model, but importantly, the validity of the LOR estimand does not depend on this assumption. As in binary and ordinal logistic regression, the null value for this estimand is 0. Since \\(-log(a/b) = log(b/a)\\) and \\(odds(Y &gt; j \\vert A = 1) = 1/odds(Y \\le j \\vert A = 1)\\), changing the sign of the log odds ratio estimator tells us about the average log odds of having scores higher than level \\(j\\) in the treatment of interest relative to the comparator group: \\[-\\theta_{LOR} = \\frac{1}{K-1} \\sum_{j=1}^{K-1} log \\left( \\frac{odds(Y &gt; j \\vert A = 1)}{odds(Y &gt; j \\vert A = 0)} \\right)\\] 3.3 Survival/Time-To-Event Outcomes When the outcome is the time from randomization until an event of interest occurs, let \\(Y\\) denote the time at which the event occurs, and \\(C\\) denote the time at which individuals would be censored. For each individual, we observe \\(\\delta_{i} = I_{\\{Y_{i} \\le C_{i}\\}}\\), whether an individual is censored or the event is observed, and \\(T_{i} = min\\{Y_{i}, C_{i}\\}\\), the time at which the event or censoring occurs. Often we are interested in a particular time window after randomization, known as the time horizon. Let \\(\\tau\\) denote the time horizon of interest for inference. 3.3.1 Survival Function When the outcome is a time-to-event, the usual target of inference is the survival function, which is the marginal probability of being event-free through time \\(t\\) if the entire population were assigned to study arm \\(A = a\\): \\[S_{0}^{(a)}(t) = Pr\\{Y &gt; t \\vert A = a\\}\\] Estimands of interest may include the difference in survival probability at time \\(\\tau\\): \\[\\theta_{DSP} = Pr(Y \\ge \\tau \\vert A = 1) - Pr(Y \\ge \\tau \\vert A = 0)\\] Instead of an additive estimand, a relative estimand can be obtained by taking the ratio of survival probabilities at time \\(\\tau\\): \\[\\theta_{RSP} = \\frac{Pr(Y \\ge \\tau \\vert A = 1)}{Pr(Y \\ge \\tau \\vert A = 0)}\\] 3.3.2 Hazard Ratio The hazard rate for individuals receiving treatment \\(A = a\\), denoted \\(h^{(a)}(t)\\), is the instantaneous rate of the event of interest at time \\(t\\) among individuals who have not yet experienced the event: \\[ h^{(a)}(t) = \\lim_{ \\Delta t \\to 0} \\frac{ P(t \\le Y &lt; t + \\Delta t \\; \\vert \\; Y \\ge t, A = a) }{\\Delta t} = \\frac{d}{dt}ln\\left(S_{0}^{(a)}(t)\\right)\\] The hazard ratio compares the ratio of two hazard functions: \\[\\theta_{HR}(t) = \\frac{h^{(1)}(t)}{h^{(0)}(t)}\\] Since both hazard function can vary over time, the true hazard ratio can vary over time. Commonly used approaches in time-to-event analysis often require the hazard rate being approximately constant over the time interval of interest, either unconditionally or conditional on covariates. It may not be known in practice whether such an assumption is reasonable, but this assumption can be empirically assessed. When the hazard ratio varies appreciably in time, methods that make a conditional or unconditional assumption of proportional hazards are less efficient and give effect estimates whose interpretation is unclear. Even when the assumption of proportional hazards is approximately true, the hazard ratio cannot easily be translated into easily communicated, meaningful effects, such as the number of years an individual can expect to be free of the event if they were assigned to one treatment or another. 3.3.3 Restricted Mean Survival Time Another estimand that may be of interest is the restricted mean survival time (RMST). This estimand is the average time-to-event (e.g. life expectancy when mortality is the event of interest) from baseline to a pre-specified point in time, denoted \\(\\tau\\). The interval \\([0, \\tau]\\) is called the time horizon. This is given by taking the area under the survival function over the time horizon: \\[RMST = E[min\\{ Y, \\tau \\} \\vert a] = \\int_{0}^{\\tau} S_{0}^{(a)}(t) dt\\] Treatments can be compared using a contrast of the RMST in the population where everyone receives treatment and that same population where everyone receives the control/comparator intervention. The difference in RMST contrast assesses the area between the survival curves under each treatment scenario. \\[\\theta_{DRMST} = E[min\\{ Y, \\tau \\} \\vert A = 1] - E[min\\{ Y, \\tau \\} \\vert A = 0]\\] A relative estimand is given by the ratio of RMST: \\[\\theta_{RRMST} = \\frac{E[min\\{ Y, \\tau \\} \\vert A = 1]}{E[min\\{ Y, \\tau \\} \\vert A = 0]}\\] "],["using_r.html", "Chapter 4 Using R 4.1 Installing Packages 4.2 Loading the Data: MISTIE-III 4.3 Assessing Baseline Balance 4.4 Visualizing Data 4.5 Fitting Regression Models 4.6 Variance Estimation 4.7 Addressing Multiplicity", " Chapter 4 Using R The R environment for Statistical Computing is a free, open source environment for managing and visualizing data and performing analyses. Its capabilities can be augmented by downloading software packages from repositories, such as the Comprehensive R Archival Network (CRAN), and more recently, GitHub. Rstudio is a powerful development environment that makes it easier to use R, expands its functionality, and allows the integration of other tools, such as Git, into an analyst’s workflow. If you are new to R or would like additional resources on using R in practice, see the appendix. In addition to R and Rstudio, we will also need to install RTools, also known as the R toolchain. These are software tools for compiling software packages. Some of the packages we use may have to be compiled after being downloaded, and RTools provides all the software needed to do so. CRAN provides instruction on which version of RTools you should use, depending on your version of R, and how to install it. 4.1 Installing Packages Once we have Rtools installed, we are ready to install the required packages from CRAN and Github. The install.packages() function in R can be used to install packages from CRAN, R-Forge, BioC, and other repositories via the command line. In Rstudio, users can also use the ‘Packages’ tab to see which packages are installed, their current version, and whether or not they have been loaded into the workspace for use. If you have installed the devtools package, devtools::install_github() can be used to install packages from Github or other version control repositories. If you would like to explore which R packages are available for a given application or research area, the CRAN Task Views, including the CRAN Clinical Trials taskview, are worth exploring. 4.1.1 Packages from CRAN Below are some of the packages that we will use from CRAN, along with a brief description of their purposes: devtools - A suite of tools for R package development cobalt - Creating tables and plots for assessing covariate balance knitr - Tools for literate programming: including code in reproducible reports margins - Calculating marginal or partial effects from regression models mgcv - Fitting generalized additive models sandwich Robust covariance matrix estimation survminer - Creating plots of time-to-event data survRM2 - Calculating the restricted mean survival time (RMST) with and without covariate adjustment. table1 - Creating simple tabulations in aggregate and by treatment arm tidyverse - An ecosystem of packages for working with data required_packages &lt;- c(&quot;devtools&quot;, &quot;cobalt&quot;, &quot;knitr&quot;, &quot;margins&quot;, &quot;mgcv&quot;, &quot;sandwich&quot;, &quot;survminer&quot;, &quot;survRM2&quot; &quot;table1&quot;, &quot;tidyverse&quot; ) packages_to_install &lt;- setdiff( x = required_packages, y = installed.packages(.Library)[, &quot;Package&quot;] ) install.packages(packages_to_install) 4.1.2 Packages from Github We will also use the following packages from Github: simul Inference based on Efficient Influence Function and Multiplier Bootstrap adjrct Doubly Robust, Efficient Estimators for Survival and Time to Event Outcomes devtools::install_github(&quot;nt-williams/simul&quot;) devtools::install_github(&quot;nt-williams/adjrct&quot;) 4.1.3 Loading Packages into Workspace Once a package has been successfully installed, we use the library() command to load it into the workspace for use: library(tidyverse) # Data manipulation: dplyr, tidyr library(table1) # Creation of Summary tables It is possible that different packages contain a function of the same name. For example, the table1 package contains the function table1(): there are other packages that also have a function named table1(), such as the furniture package. If both of these packages are loaded, it can cause confusion about which version should be used when table1() is called. R will warn when such conflicts can arise, but as a best practice, it is useful to specify the package and function as as follows: table1::table1(). This makes code easier to use and potentially reduces ambiguity. 4.2 Loading the Data: MISTIE-III The data dictionary and more information about the MISTIE III study were presented earlier. All of the data needed for the examples are available on the web. To load these, we create a file connection to the URL using the url() function, and then use read.csv() to read in the comma separated values (CSV) file. To load data from a local file path, the file.path() function is useful for creating file paths. We can start by loading the simulated MISTIE III data. Once we have loaded the full data, we can use dplyr::slice to take the first 500 rows. data_url &lt;- paste0(&quot;https://github.com/jbetz-jhu/CovariateAdjustmentTutorial&quot;, &quot;/raw/main/Simulated_MISTIE_III_v1.2.csv&quot;) sim_miii_full &lt;- read.csv(file = url(data_url)) # Read in data: Recast categorical variables as factors sim_miii_full &lt;- sim_miii_full %&gt;% dplyr::tibble() %&gt;% dplyr::mutate( # Convert variables from binary indicators to labeled categorical variables male = factor( x = male, levels = 0:1, labels = c(&quot;0. Female&quot;, &quot;1. Male&quot;) ), across( .cols = all_of( x = c(&quot;hx_cvd&quot;, &quot;hx_hyperlipidemia&quot;, &quot;on_anticoagulants&quot;, &quot;on_antiplatelets&quot;) ), .fns = function(x) factor(x, levels = 0:1, labels = c(&quot;0. No&quot;, &quot;1. Yes&quot;)) ), # Convert GCS and MRS variables from character data to categorical variables across( .cols = starts_with(&quot;gcs&quot;) | starts_with(&quot;mrs&quot;), .fns = factor ), ich_location = factor( x = ich_location, levels = c(&quot;Deep&quot;, &quot;Lobar&quot;) ), arm = factor( x = arm, levels = c(&quot;medical&quot;, &quot;surgical&quot;) ), tx = 1*(arm == &quot;surgical&quot;) ) # Take the first 500 rows sim_miii &lt;- sim_miii_full %&gt;% dplyr::slice(1:500) Other useful functions include: head()/tail() - Looking at the first \\(n\\) rows of a dataset nrow()/ncol() - Counting the rows/columns of a dataset colnames()/rownames() - Getting the row/column names of a dataset head(sim_miii) ## # A tibble: 6 × 22 ## sim_participant_id age male hx_cvd hx_hyperlipidemia on_anticoagulants ## &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 52 1. Male 0. No 0. No 0. No ## 2 2 74 0. Female 0. No 1. Yes 1. Yes ## 3 3 62 1. Male 0. No 1. Yes 0. No ## 4 4 81 0. Female 0. No 0. No 0. No ## 5 5 73 0. Female 0. No 1. Yes 0. No ## 6 6 60 1. Male 1. Yes 1. Yes 0. No ## # ℹ 16 more variables: on_antiplatelets &lt;fct&gt;, ich_location &lt;fct&gt;, ## # ich_s_volume &lt;dbl&gt;, ivh_s_volume &lt;int&gt;, gcs_category &lt;fct&gt;, arm &lt;fct&gt;, ## # ich_eot_volume &lt;dbl&gt;, mrs_30d &lt;fct&gt;, mrs_30d_complete &lt;fct&gt;, ## # mrs_180d &lt;fct&gt;, mrs_180d_complete &lt;fct&gt;, mrs_365d &lt;fct&gt;, ## # mrs_365d_complete &lt;fct&gt;, days_on_study &lt;int&gt;, died_on_study &lt;int&gt;, tx &lt;dbl&gt; nrow(sim_miii) ## [1] 500 ncol(sim_miii) ## [1] 22 colnames(sim_miii) ## [1] &quot;sim_participant_id&quot; &quot;age&quot; &quot;male&quot; ## [4] &quot;hx_cvd&quot; &quot;hx_hyperlipidemia&quot; &quot;on_anticoagulants&quot; ## [7] &quot;on_antiplatelets&quot; &quot;ich_location&quot; &quot;ich_s_volume&quot; ## [10] &quot;ivh_s_volume&quot; &quot;gcs_category&quot; &quot;arm&quot; ## [13] &quot;ich_eot_volume&quot; &quot;mrs_30d&quot; &quot;mrs_30d_complete&quot; ## [16] &quot;mrs_180d&quot; &quot;mrs_180d_complete&quot; &quot;mrs_365d&quot; ## [19] &quot;mrs_365d_complete&quot; &quot;days_on_study&quot; &quot;died_on_study&quot; ## [22] &quot;tx&quot; 4.3 Assessing Baseline Balance While randomization will tend to produce treatment groups that are similar on both measured and unmeasured factors, there will always be some degree of imbalance between groups in some characteristics. It is important to remember that these differences only represent confounding if groups are imbalanced on variables that are associated with the outcome. To assess the degree of imbalance, we can tabulate characteristics by treatment arm, and compute standardized differences to get a scale-free measure of the magnitude of imbalance. table1( ~ age + male + on_antiplatelets + ich_location + ich_s_volume + ivh_s_volume + gcs_category | arm, data = sim_miii ) medical(N=250) surgical(N=250) Overall(N=500) age Mean (SD) 60.6 (12.9) 60.1 (12.5) 60.3 (12.7) Median [Min, Max] 62.0 [28.0, 85.0] 61.0 [35.0, 88.0] 62.0 [28.0, 88.0] male 0. Female 100 (40.0%) 94 (37.6%) 194 (38.8%) 1. Male 150 (60.0%) 156 (62.4%) 306 (61.2%) on_antiplatelets 0. No 179 (71.6%) 164 (65.6%) 343 (68.6%) 1. Yes 71 (28.4%) 86 (34.4%) 157 (31.4%) ich_location Deep 155 (62.0%) 159 (63.6%) 314 (62.8%) Lobar 95 (38.0%) 91 (36.4%) 186 (37.2%) ich_s_volume Mean (SD) 47.9 (17.3) 47.4 (16.2) 47.7 (16.7) Median [Min, Max] 45.9 [15.2, 112] 47.0 [12.9, 106] 46.3 [12.9, 112] ivh_s_volume Mean (SD) 2.46 (3.96) 2.09 (3.40) 2.28 (3.69) Median [Min, Max] 0 [0, 32.0] 0 [0, 18.0] 0 [0, 32.0] gcs_category 1. Severe (3-8) 71 (28.4%) 63 (25.2%) 134 (26.8%) 2. Moderate (9-12) 111 (44.4%) 102 (40.8%) 213 (42.6%) 3. Mild (13-15) 68 (27.2%) 85 (34.0%) 153 (30.6%) This allows us to compare the means, medians, frequencies, and ranges of variables between groups. library(cobalt) cobalt::bal.tab( x = # Only tabulate baseline variables sim_miii %&gt;% dplyr::select( dplyr::all_of( x = c(&quot;age&quot;, &quot;male&quot;, &quot;on_antiplatelets&quot;, &quot;ich_location&quot;, &quot;ich_s_volume&quot;, &quot;ivh_s_volume&quot;, &quot;gcs_category&quot;) ) ), treat = sim_miii$arm, # Compute standardized differences for both binary and continuous variables binary = &quot;std&quot;, continuous = &quot;std&quot; ) ## Balance Measures ## Type Diff.Un ## age Contin. -0.0362 ## male_1. Male Binary 0.0493 ## on_antiplatelets_1. Yes Binary 0.1295 ## ich_location_Lobar Binary -0.0331 ## ich_s_volume Contin. -0.0292 ## ivh_s_volume Contin. -0.1019 ## gcs_category_1. Severe (3-8) Binary -0.0723 ## gcs_category_2. Moderate (9-12) Binary -0.0729 ## gcs_category_3. Mild (13-15) Binary 0.1480 ## ## Sample sizes ## medical surgical ## All 250 250 4.4 Visualizing Data One of the many strength of R is the powerful and flexible data visualization tools in its software ecosystem: see the R Graph Gallery for some examples. The ggplot2 package, and some related packages like survminer, are useful for assessing baseline balance and visualizing outcome data. For example, we may want to assess the cumulative distribution of a baseline covariate instead of just checking the summary statistics. library(ggplot2) ggplot( data = sim_miii, aes( x = age ) ) + stat_ecdf( alpha = 0.6, ) + stat_ecdf( aes(color = arm), alpha = 0.6, ) + theme_bw() Or we may want to create a plot of the Kaplan-Meier estimate of the survival function (see the estimands section for its definition). library(survival) library(survminer) # Create a &#39;survival object&#39; from event times and indicators miii_surv &lt;- with(sim_miii, survival::Surv( time = days_on_study, event = died_on_study ) ) # Use survfit to calculate survival data time_to_death_km &lt;- survival::survfit( formula = miii_surv ~ arm, data = sim_miii ) # Create the Kaplan-Meier Plot survminer::ggsurvplot( fit = time_to_death_km, conf.int = TRUE, risk.table = TRUE, xlab = &quot;Days&quot;, ylab = &quot;Survival probability&quot; ) From this plot, we can see that most of the mortality occurs soon after randomization, with greater mortality in the medical management arm early in the trial. After the first 90 days, the rate of events decreases in both arms. 4.5 Fitting Regression Models While covariate adjustment does involve regression modeling, an in-depth discussion of regression modeling is not needed for the purposes of implementing covariate adjustment. For those wanting a more in-depth presentation of fitting generalized linear models (GLMs) in R see Dobson &amp; Barnett (2018). 4.5.1 Generalized Linear Model Fitting a GLM in R is done using the glm() function. For example if we wanted to model the probability of being assigned to the surgical arm in MISTIE III by an individual’s age, ICH volume, and IVH volume, the code is as follows: pr_mis_glm &lt;- glm( formula = tx ~ age + ich_s_volume + ivh_s_volume, data = sim_miii, family = binomial(link = &quot;logit&quot;) ) Once the model has been fit, we can use it for creating summary tables, calculating confidence intervals, or generating fitted values for a new dataset: # Produce GLM Summary Table summary(pr_mis_glm) ## ## Call: ## glm(formula = tx ~ age + ich_s_volume + ivh_s_volume, family = binomial(link = &quot;logit&quot;), ## data = sim_miii) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.3090527 0.5028292 0.615 0.539 ## age -0.0034148 0.0070781 -0.482 0.629 ## ich_s_volume -0.0008168 0.0054271 -0.151 0.880 ## ivh_s_volume -0.0282560 0.0249439 -1.133 0.257 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 693.15 on 499 degrees of freedom ## Residual deviance: 691.58 on 496 degrees of freedom ## AIC: 699.58 ## ## Number of Fisher Scoring iterations: 3 # Calculate Confidence Intervals for Coefficients confint(pr_mis_glm) ## 2.5 % 97.5 % ## (Intercept) -0.67578350 1.29885632 ## age -0.01732742 0.01045830 ## ich_s_volume -0.01148661 0.00983616 ## ivh_s_volume -0.07808960 0.02017825 # Calculate fitted probabilities for new data: # 1. 65 years old, 30 mL ICH, 0 mL IVH # 2. 70 years old, 50 mL ICH, 15 mL IVH predict( object = pr_mis_glm, newdata = data.frame( age = c(65, 70), ich_s_volume = c(30, 50), ivh_s_volume = c(0, 15) ), type = &quot;response&quot; ) ## 1 2 ## 0.5156414 0.4025950 4.5.2 Logrank Test and Cox Proportional Hazards Model For time-to-event outcomes, such as mortality, the logrank test and Cox Proportional Hazards (PH) model are commonly used analytic approaches. Using the survival object miii_surv created earlier using survival::Surv(), we can pass this object to other functions to perform the logrank test (using survival::survdiff) and fit the Cox PH model (using survival::coxph). mortality_logrank &lt;- survival::survdiff( formula = miii_surv ~ arm, data = sim_miii ) mortality_logrank ## Call: ## survival::survdiff(formula = miii_surv ~ arm, data = sim_miii) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## arm=medical 250 64 49.6 4.19 8.06 ## arm=surgical 250 40 54.4 3.82 8.06 ## ## Chisq= 8.1 on 1 degrees of freedom, p= 0.005 For the Cox PH model, the ties = \"efron\" argument specifies how tied survival times are addressed, and the robust = TRUE computes robust estimates of the covariance matrix of regression coefficients: mortality_cox &lt;- survival::coxph( formula = miii_surv ~ arm, data = sim_miii, ties = &quot;efron&quot;, robust = TRUE ) summary(mortality_cox) ## Call: ## survival::coxph(formula = miii_surv ~ arm, data = sim_miii, ties = &quot;efron&quot;, ## robust = TRUE) ## ## n= 500, number of events= 104 ## ## coef exp(coef) se(coef) robust se z Pr(&gt;|z|) ## armsurgical -0.5670 0.5672 0.2016 0.1995 -2.841 0.00449 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## armsurgical 0.5672 1.763 0.3836 0.8387 ## ## Concordance= 0.575 (se = 0.024 ) ## Likelihood ratio test= 8.16 on 1 df, p=0.004 ## Wald test = 8.07 on 1 df, p=0.004 ## Score (logrank) test = 8.12 on 1 df, p=0.004, Robust = 8.03 p=0.005 ## ## (Note: the likelihood ratio and score tests assume independence of ## observations within a cluster, the Wald and robust score tests do not). This model assumes that the ratio of the rates of events between the treatment and control arm is approximately constant over time. We can assess using the survival::cox.zph function: # Test Proportionality Assumption: Schoenfeld Residuals cox.zph(fit = mortality_cox) ## chisq df p ## arm 9.85 1 0.0017 ## GLOBAL 9.85 1 0.0017 # Plot smoothed residuals plot(cox.zph(fit = mortality_cox)) abline(h = 0, col = &quot;red&quot;) From the plot, we can see that the hazard ratio is initially negative, but increases towards zero as time goes on. It seems as if the treatment has a beneficial effect on mortality for a few weeks after randomization, which then diminishes. This is a violation of the proportional hazards assumption, which is further discussed in the chapter on estimands. 4.6 Variance Estimation 4.6.1 Robust Standard Errors Most of the standard errors reported in software, such as those returned by vcov(), are model-based estimates of the standard error, which assume that the model is correctly specified. Robust or “Sandwich” standard errors can be used to obtain a consistent estimate of the standard error in such cases. Note that robust estimates of standard errors are different from robust estimates of the regression coefficients themselves. The sandwich::vcovHC() function can be used to obtain different types of robust standard errors: library(sandwich) # Model-based standard errors vcov(object = pr_mis_glm) ## (Intercept) age ich_s_volume ivh_s_volume ## (Intercept) 0.252837171 -2.993310e-03 -1.285278e-03 -1.316243e-03 ## age -0.002993310 5.009960e-05 -1.254035e-06 1.346355e-05 ## ich_s_volume -0.001285278 -1.254035e-06 2.945316e-05 -1.887093e-05 ## ivh_s_volume -0.001316243 1.346355e-05 -1.887093e-05 6.221973e-04 # Robust standard errors sandwich::vcovHC(x = pr_mis_glm, type = &quot;HC3&quot;) ## (Intercept) age ich_s_volume ivh_s_volume ## (Intercept) 0.263262683 -3.112090e-03 -1.349840e-03 -0.0014409131 ## age -0.003112090 5.179135e-05 -9.534434e-07 0.0000144568 ## ich_s_volume -0.001349840 -9.534434e-07 3.044098e-05 -0.0000171047 ## ivh_s_volume -0.001440913 1.445680e-05 -1.710470e-05 0.0006168577 These can be passed as an argument to other functions for computing confidence intervals for contrasts and marginal means. 4.6.2 Bootstrap Estimator The bootstrap procedure uses resampling to obtain an estimate of the variance of the sampling distribution of an estimator. In R, this is done using the boot package, which is part of base R. First, we need to write a function to produce the statistic of interest. In this case, we will bootstrap the Mann-Whitney U statistic: see the estimands for more information on this estimand. The first argument to this function must be the data, and the second argument should be a vector of indices for our bootstrap sample, and any other arguments can be supplied thereafter. # 1. Write a function that produces the test statistic: wilcox_to_auc &lt;- function(data, indices = NULL, formula){ # Input data must be a data.frame if(!all(class(data) == &quot;data.frame&quot;)){ stop(&quot;`data` must be a data.frame: use `as.data.frame()` for a tibble.&quot;) } # If bootstrap indices not supplied, use entire dataset if(is.null(indices)) indices &lt;- 1:nrow(data) # Extract Outcome/Treatment from Formula outcome &lt;- all.vars(update(formula, . ~ 0)) treatment &lt;- all.vars(update(formula, 0 ~ .)) stopifnot(length(treatment) == 1) # Convert outcome to numeric using levels: Assumes levels are ordered if(!is.numeric(data[, outcome])){ data[, outcome] &lt;- as.numeric(data[, outcome]) } # Run Wilcoxon Rank Sum on data using the bootstrap indices wrst_result &lt;- wilcox.test( formula = formula, data = data[indices,] ) # Compute AUC statistic return(wrst_result$statistic/prod(table(data[indices, treatment]))) } Now, we call this function using boot, passing any arguments required: boot will resample the data, and pass the indices to the function, and evaluate the result. From the result, we can calculate the standard error or estimate of the bootstrap distribution: # Perform 10,000 bootstraps of data n_boot_samples &lt;- 10000 mrs_365d_auc_boot &lt;- boot::boot( data = as.data.frame(sim_miii), statistic = wilcox_to_auc, R = n_boot_samples, formula = mrs_365d ~ arm ) # Bootstrap Standard Error sd(mrs_365d_auc_boot$t[,1]) ## [1] 0.02515884 # Bootstrap Variance (SE^2) var(mrs_365d_auc_boot$t[,1]) ## [1] 0.000632967 Once the bootstrap procedure is complete, we can view and summarize the results. # Produce a histogram and quantile-quantile plot plot(mrs_365d_auc_boot) mrs_365d_auc_boot_ci &lt;- boot::boot.ci( boot.out = mrs_365d_auc_boot, conf = 0.95, type = &quot;bca&quot;, index = 1 ) # boot.ci result mrs_365d_auc_boot_ci ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = mrs_365d_auc_boot, conf = 0.95, type = &quot;bca&quot;, ## index = 1) ## ## Intervals : ## Level BCa ## 95% ( 0.5022, 0.6005 ) ## Calculations and Intervals on Original Scale # Extract the lower/upper confidence limits mrs_365d_auc_boot_ci_result &lt;- tail(x = mrs_365d_auc_boot_ci$bca[1,], n = 2) # Print out result mrs_365d_auc_boot_ci_result ## ## 0.5022205 0.6005486 The most straightforward way to calculate a p-value using the bootstrap involves finding the smallest value of \\(\\alpha\\) at which the \\(100(1 - \\alpha)\\%\\) confidence interval does not contain the null value, resulting in a rejection of the null hypothesis. This can be done using a binary search algorithm. 4.7 Addressing Multiplicity For pivotal trials, it is often required to have an analysis plan that control the probability of rejecting at least one truly null hypothesis, also known as the familywise type I error rate (FWER). When there are more than one primary comparison of interest, due to having multiple pairwise treatment contrasts, endpoints, sequential analyses, or timepoints, a strategy must be used to control the FWER at a pre-specified level. A later chapter is devoted to Group Sequential Designs, a methodology for doing pre-planned analyses that allow stopping a randomized trial early for success or futility. When all hypotheses are tested simultaneously, the multcomp package allows for decisions and confidence intervals that appropriately control the FWER. When there is a specific ordering to how hypotheses are tested, the gMCP package implements graphical methods for addressing multiple comparisons. We will focus mainly on controlling the FWER for one pairwise treatment comparison on a single endpoint, with one or more pre-planned analyses. References "],["applying-covariate-adjustment-ancova-g-computation-dr-wls.html", "Chapter 5 Applying Covariate Adjustment: ANCOVA, G-Computation, DR-WLS 5.1 Calculating Precision Gain 5.2 Missing Outcome Data: Inverse Weighting", " Chapter 5 Applying Covariate Adjustment: ANCOVA, G-Computation, DR-WLS In this next section, we will use R to produce the ANCOVA estimator, get an estimate of its standard error, and compare its precision to an unadjusted analysis using R. For this example we will use the Buprenorphine tapering trial data (CTN-03). data_url &lt;- &quot;https://github.com/jbetz-jhu/CovariateAdjustmentTutorial/raw/main/SIMULATED_CTN03_220506.Rdata&quot; load(file = url(data_url)) This .Rdata file contains two datasets: ctn03_sim, which has no missing data, and ctn03_sim_mar, where a simulated missing data mechanism has been applied. # 1. Regress final outcome (VAS at end of taper) on treatment assignment and # outcome assessed at baseline (VAS at baseline) vas_ancova_1 &lt;- lm( formula = vas_crave_opiates_eot ~ arm + vas_crave_opiates_bl, data = ctn03_sim_mar ) summary(vas_ancova_1) ## ## Call: ## lm(formula = vas_crave_opiates_eot ~ arm + vas_crave_opiates_bl, ## data = ctn03_sim_mar) ## ## Residuals: ## Min 1Q Median 3Q Max ## -54.193 -21.747 -6.874 15.151 71.637 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.05363 2.27269 10.584 &lt; 2e-16 *** ## arm7-day -2.17932 2.82195 -0.772 0.44 ## vas_crave_opiates_bl 0.35910 0.07595 4.728 3.29e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 25.96 on 350 degrees of freedom ## (163 observations deleted due to missingness) ## Multiple R-squared: 0.06006, Adjusted R-squared: 0.05469 ## F-statistic: 11.18 on 2 and 350 DF, p-value: 1.962e-05 Note that the standard error reported by lm() is the model-based standard error, not a robust standard error or bootstrap standard error. # 2. Generate predictions based on fitted model: # Their predicted outcome if they were assigned to treatment (7-day taper) # Their predicted outcome if they were assigned to control (28-day taper) expected_vas_7_day &lt;- predict( object = vas_ancova_1, newdata = within( data = ctn03_sim_mar, expr = {arm = &quot;7-day&quot;} ), type = &quot;response&quot; ) expected_vas_28_day &lt;- predict( object = vas_ancova_1, newdata = within( data = ctn03_sim_mar, expr = {arm = &quot;28-day&quot;} ), type = &quot;response&quot; ) data.frame( expected_vas_7_day, expected_vas_28_day, ctn03_sim_mar[, c(&quot;arm&quot;, &quot;vas_crave_opiates_eot&quot;, &quot;vas_crave_opiates_bl&quot;)] ) %&gt;% head() ## expected_vas_7_day expected_vas_28_day arm vas_crave_opiates_eot ## 1 27.97901 30.15833 7-day NA ## 2 24.38801 26.56733 7-day 5 ## 3 32.64731 34.82663 7-day 31 ## 4 24.38801 26.56733 7-day 40 ## 5 28.69721 30.87653 7-day 51 ## 6 26.18351 28.36283 7-day 1 ## vas_crave_opiates_bl ## 1 17 ## 2 7 ## 3 30 ## 4 7 ## 5 19 ## 6 12 Once we’ve generated these predictions, we can compute the ANCOVA estimate of the average treatment effect: mean(expected_vas_7_day) ## [1] 26.88292 mean(expected_vas_28_day) ## [1] 29.06224 vas_ancova_estimate &lt;- mean(expected_vas_7_day) - mean(expected_vas_28_day) vas_ancova_estimate ## [1] -2.179321 Notice this is the exact same as the regression coefficient for arm in the regression model, since there’s no treatment-covariate interactions and an identity link function. We can obtain the standard error of the estimate two ways. The first way is using the margins::margins() command, using the robust standard errors from sandwich::vcovHC: vas_ancova_margins &lt;- margins::margins( model = vas_ancova_1, # Specify treatment variable variables = &quot;arm&quot;, # Convert to outcome scale, not link scale type = &quot;response&quot;, # Obtain robust standard errors vcov = sandwich::vcovHC(x = vas_ancova_1, type = &quot;HC3&quot;) ) summary(object = vas_ancova_margins, level = 0.95) ## factor AME SE z p lower upper ## arm7-day -2.1793 2.8680 -0.7599 0.4473 -7.8005 3.4418 You’ll see that we now have a standard error, p-value under the hypothesis that the marginal effect is 0, and a 95% Confidence Interval for the estimate. Another way to obtain these is the bias corrected and accelerated (BCa) non-parametric bootstrap: # Write a function to produce the ANCOVA estimate margins_fun &lt;- function(data, indices = NULL, formula, family, term){ # Input data must be a data.frame if(!all(class(data) == &quot;data.frame&quot;)){ stop(&quot;`data` must be a data.frame: use `as.data.frame()` for a tibble.&quot;) } # If bootstrap indices not supplied, use entire dataset if(is.null(indices)) indices &lt;- 1:nrow(data) data &lt;- data[indices,] glm_fit &lt;- glm( formula = formula, family = family, data = data ) tx_levels &lt;- levels(data[, term]) e_y_1 &lt;- predict( object = glm_fit, newdata = within( data, expr = assign(x = term, value = tx_levels[2]) ), type = &quot;response&quot; ) e_y_0 &lt;- predict( object = glm_fit, newdata = within( data, expr = assign(x = term, value = tx_levels[1]) ), type = &quot;response&quot; ) return(mean(e_y_1) - mean(e_y_0)) } vas_ancova_boot &lt;- boot::boot( data = ctn03_sim_mar, statistic = margins_fun, R = 10000, formula = vas_crave_opiates_eot ~ arm + vas_crave_opiates_bl, family = gaussian(link = &quot;identity&quot;), term = &quot;arm&quot; ) vas_ancova_boot ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot::boot(data = ctn03_sim_mar, statistic = margins_fun, R = 10000, ## formula = vas_crave_opiates_eot ~ arm + vas_crave_opiates_bl, ## family = gaussian(link = &quot;identity&quot;), term = &quot;arm&quot;) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* -2.179321 0.0631477 2.816499 We can extract the standard error and produce a 95% confidence interval: # Bootstrap Standard Error sd(vas_ancova_boot$t[,1]) ## [1] 2.816499 vas_ancova_boot_ci &lt;- boot::boot.ci( boot.out = vas_ancova_boot, conf = 0.95, type = &quot;bca&quot; ) vas_ancova_boot_ci ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = vas_ancova_boot, conf = 0.95, type = &quot;bca&quot;) ## ## Intervals : ## Level BCa ## 95% (-7.673, 3.237 ) ## Calculations and Intervals on Original Scale We can compare these to the results from margins::margins(): # Margins SE ancova_margins_se &lt;- summary(object = vas_ancova_margins, level = 0.95)$SE ancova_margins_se ## [1] 2.867989 # Bootstrap SE ancova_boot_se &lt;- sd(vas_ancova_boot$t[,1]) ancova_boot_se ## [1] 2.816499 # Compare as ratio ancova_margins_se/ancova_boot_se ## [1] 1.018282 The standard errors are nearly identical using the estimated marginal effect with robust standard errors or the bootstrap. 5.1 Calculating Precision Gain Asymptotically, the ANCOVA estimate should have precision that is equal or better than an unadjusted analysis. We can calculate the precision gain by taking the ratio of the variances, i.e. the squared standard errors, of the adjusted estimator to the unadjusted estimator. vas_t_test &lt;- t.test( formula = vas_crave_opiates_eot ~ arm, data = ctn03_sim_mar, var.equal = FALSE ) vas_t_test ## ## Welch Two Sample t-test ## ## data: vas_crave_opiates_eot by arm ## t = 0.093598, df = 316.77, p-value = 0.9255 ## alternative hypothesis: true difference in means between group 28-day and group 7-day is not equal to 0 ## 95 percent confidence interval: ## -5.418971 5.960313 ## sample estimates: ## mean in group 28-day mean in group 7-day ## 28.01325 27.74257 # Get unadjusted standard error t_test_se &lt;- vas_t_test$stderr t_test_se ## [1] 2.891841 The precision gain is equal to the ratio of the variance, which is the square of the standard error: # Percentage reduction in variance adjusting for baseline outcome 100*(1 - (ancova_margins_se/t_test_se)^2) ## [1] 1.642819 The precision gain in this particular example is rather small, but we have only adjusted for one potential covariate. However, there are several other baseline variables we did not include in the model. We can see if the gain in precision is larger when these other variables are included. We should also include the stability dose in the outcome, since randomization was stratified by this variable: vas_ancova_2 &lt;- glm( formula = vas_crave_opiates_eot ~ arm + vas_crave_opiates_bl + stability_dose + arsw_score_bl + cows_total_score_bl + vas_current_withdrawal_bl + vas_study_tx_help_bl + uds_any_positive_bl, data = ctn03_sim_mar ) vas_ancova_margins_2 &lt;- margins::margins( model = vas_ancova_2, # Specify treatment variable variables = &quot;arm&quot;, # Convert to outcome scale, not link scale type = &quot;response&quot;, # Obtain robust standard errors vcov = sandwich::vcovHC(x = vas_ancova_2, type = &quot;HC3&quot;) ) ancova_margins_2_se &lt;- summary(object = vas_ancova_margins_2, level = 0.95)$SE # Percentage reduction in variance adjusting for baseline outcome 100*(1 - (ancova_margins_2_se/t_test_se)^2) ## [1] 7.590248 Including these other covariates leads to a larger gain in precision. 5.2 Missing Outcome Data: Inverse Weighting So far, we have not discussed the issue of missing outcome data. While the G-computation estimator is robust to arbitrary model misspecification, it is only valid if data are missing completely at random (MCAR): missingness is unrelated to either the observed or unobserved data. If this were the case, we should not see any association between the baseline covariates and missingness in the VAS opiate craving scores at the end-of-taper visit. We can assess this using a generalized additive model (GAM): vas_mar_glm &lt;- mgcv::gam( formula = is.na(vas_crave_opiates_eot) ~ arm + stability_dose + s(age) + sex + s(vas_crave_opiates_bl) + s(arsw_score_bl) + s(cows_total_score_bl) + s(vas_current_withdrawal_bl) + s(vas_study_tx_help_bl) + uds_any_positive_bl, family = binomial(link = &quot;logit&quot;), data = ctn03_sim_mar ) summary(vas_mar_glm) ## ## Family: binomial ## Link function: logit ## ## Formula: ## is.na(vas_crave_opiates_eot) ~ arm + stability_dose + s(age) + ## sex + s(vas_crave_opiates_bl) + s(arsw_score_bl) + s(cows_total_score_bl) + ## s(vas_current_withdrawal_bl) + s(vas_study_tx_help_bl) + ## uds_any_positive_bl ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.80921 0.56096 -3.225 0.00126 ** ## arm7-day -1.01189 0.20978 -4.824 1.41e-06 *** ## stability_dose16 mg 1.74449 0.57581 3.030 0.00245 ** ## stability_dose24 mg 1.46083 0.56381 2.591 0.00957 ** ## sexFemale 0.11516 0.22339 0.516 0.60620 ## uds_any_positive_blPositive -0.07541 0.21609 -0.349 0.72711 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(age) 1.758 2.210 1.947 0.4526 ## s(vas_crave_opiates_bl) 1.000 1.000 1.436 0.2308 ## s(arsw_score_bl) 1.000 1.000 7.327 0.0068 ** ## s(cows_total_score_bl) 3.110 3.839 6.372 0.1944 ## s(vas_current_withdrawal_bl) 2.120 2.692 7.016 0.0561 . ## s(vas_study_tx_help_bl) 1.000 1.000 1.542 0.2143 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.0853 Deviance explained = 9.61% ## UBRE = 0.18961 Scale est. = 1 n = 516 The GAM model shows that, all other things being equal, missingness was lower in the 7-day arm, higher in individuals on 16 or 24 mg stability doses, and was associated with some of the baseline outcome measures. In the next section, we will discuss methods that can be robust to model misspecification and do not require the MCAR assumption. "],["doublyrobust.html", "Chapter 6 Doubly Robust Estimators 6.1 Augmented Inverse Probability Weighted Estimation 6.2 Targeted Maximum Likelihood Estimation", " Chapter 6 Doubly Robust Estimators 6.1 Augmented Inverse Probability Weighted Estimation 6.2 Targeted Maximum Likelihood Estimation "],["groupsequential.html", "Chapter 7 Group Sequential Designs 7.1 Group Sequential Designs 7.2 Power &amp; Sample Size Calculations 7.3 Analyses: Unadjusted and Adjusted", " Chapter 7 Group Sequential Designs 7.1 Group Sequential Designs 7.2 Power &amp; Sample Size Calculations 7.3 Analyses: Unadjusted and Adjusted "],["informationadaptive.html", "Chapter 8 Information Adaptive Designs 8.1 Information Adaptive Designs 8.2 Power &amp; Sample Size Calculations 8.3 Analyses: Unadjusted and Adjusted", " Chapter 8 Information Adaptive Designs 8.1 Information Adaptive Designs 8.2 Power &amp; Sample Size Calculations 8.3 Analyses: Unadjusted and Adjusted "],["recommendations.html", "Chapter 9 Guidance on Applying Covariate Adjustment 9.1 Statistical Analysis Plans 9.2 Sample size for adjusted vs. unadjusted 9.3 Number of parameters vs. sample size", " Chapter 9 Guidance on Applying Covariate Adjustment 9.1 Statistical Analysis Plans 9.2 Sample size for adjusted vs. unadjusted 9.3 Number of parameters vs. sample size "],["appendices.html", "Chapter 10 Appendices 10.1 Appendix A: Glossary of Terms 10.2 Appendix B: Resources for Using R", " Chapter 10 Appendices 10.1 Appendix A: Glossary of Terms 10.2 Appendix B: Resources for Using R There are a lot of excellent existing resources on learning and using R in practice. A non-exhaustive list of these resources are below. Getting Started R Project: An Introduction to R Rstudio Educational Resources Rstudio Primers R for Data Science Introduction to Data Science Exploratory Data Analysis with R Coursera Data Science Foundations using R edX Data Science: R Basics Happy Git and GitHub for the useR The R Graph Gallery Quick References RStudio Cheat Sheets Programming &amp; Advanced Topics Hands On Programming with R Advanced R "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
